{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI LLM Q&A Demo\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's language models for basic question-answering tasks. You'll learn different prompting techniques and practice with hands-on exercises.\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up OpenAI API client\n",
    "- Perform basic Q&A inference\n",
    "- Explore different prompting strategies\n",
    "- Practice with real-world examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your OpenAI API key. **Important**: Never hardcode your API key in notebooks. Use environment variables or a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY')  # Make sure to set this in your .env file\n",
    ")\n",
    "\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Q&A Function\n",
    "\n",
    "Let's create a simple function to ask questions to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=150):\n",
    "    \"\"\"\n",
    "    Ask a question to OpenAI's LLM and get a response\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask\n",
    "        model (str): The model to use (default: gpt-3.5-turbo)\n",
    "        temperature (float): Controls randomness (0-1)\n",
    "        max_tokens (int): Maximum response length\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Q&A Examples\n",
    "\n",
    "Let's try some simple questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple factual question\n",
    "question1 = \"What is the capital of France?\"\n",
    "answer1 = ask_question(question1)\n",
    "print(f\"Q: {question1}\")\n",
    "print(f\"A: {answer1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math question\n",
    "question2 = \"What is 15% of 240?\"\n",
    "answer2 = ask_question(question2)\n",
    "print(f\"Q: {question2}\")\n",
    "print(f\"A: {answer2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation question\n",
    "question3 = \"Explain photosynthesis in simple terms.\"\n",
    "answer3 = ask_question(question3, max_tokens=200)\n",
    "print(f\"Q: {question3}\")\n",
    "print(f\"A: {answer3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Prompting Techniques\n",
    "\n",
    "### 4.1 System Messages for Role-Based Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_role(question, role=\"helpful assistant\", model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Ask a question with a specific role/persona\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are a {role}.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Example: Ask the same question with different roles\n",
    "question = \"How should I invest my money?\"\n",
    "\n",
    "print(\"=== Financial Advisor Role ===\")\n",
    "advisor_response = ask_with_role(question, \"professional financial advisor\")\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {advisor_response}\\n\")\n",
    "\n",
    "print(\"=== Conservative Parent Role ===\")\n",
    "parent_response = ask_with_role(question, \"conservative parent giving advice to their child\")\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {parent_response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Few-Shot Learning Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_qa(question, examples=None, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Use few-shot learning with examples\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    if examples:\n",
    "        # Add system message with examples\n",
    "        example_text = \"Here are some examples of how to answer questions:\\n\\n\"\n",
    "        for ex_q, ex_a in examples:\n",
    "            example_text += f\"Q: {ex_q}\\nA: {ex_a}\\n\\n\"\n",
    "        example_text += \"Now answer the following question in a similar style:\"\n",
    "        messages.append({\"role\": \"system\", \"content\": example_text})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Example: Teach the model to answer in a specific format\n",
    "examples = [\n",
    "    (\"What is machine learning?\", \"Machine learning is like teaching computers to learn patterns from data, similar to how humans learn from experience.\"),\n",
    "    (\"What is artificial intelligence?\", \"Artificial intelligence is like giving computers the ability to think and make decisions, similar to human intelligence but using algorithms.\")\n",
    "]\n",
    "\n",
    "question = \"What is deep learning?\"\n",
    "answer = few_shot_qa(question, examples)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_qa(question, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Use chain of thought prompting for step-by-step reasoning\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{question}\n",
    "\n",
    "Let me think through this step by step:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,  # Lower temperature for more consistent reasoning\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Example: Complex reasoning question\n",
    "complex_question = \"If a train travels 120 miles in 2 hours, and then increases its speed by 25% for the next 3 hours, how far will it travel in total?\"\n",
    "answer = chain_of_thought_qa(complex_question)\n",
    "print(f\"Q: {complex_question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Basic Q&A\n",
    "Try asking different types of questions using the `ask_question` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Replace with your own questions\n",
    "my_questions = [\n",
    "    \"What is the largest planet in our solar system?\",\n",
    "    \"How do you make a simple pasta sauce?\",\n",
    "    \"Explain the concept of gravity in one sentence.\"\n",
    "]\n",
    "\n",
    "for q in my_questions:\n",
    "    answer = ask_question(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Temperature Experimentation\n",
    "See how temperature affects creativity in responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Test different temperatures\n",
    "creative_question = \"Write a short story about a robot learning to paint.\"\n",
    "\n",
    "temperatures = [0.1, 0.5, 0.9]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"=== Temperature: {temp} ===\")\n",
    "    answer = ask_question(creative_question, temperature=temp, max_tokens=200)\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create Your Own Role-Based Assistant\n",
    "Design a specialized assistant for a specific domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create your own specialized assistant\n",
    "def create_specialist_assistant(domain, question):\n",
    "    \"\"\"\n",
    "    Create a specialist assistant for a specific domain\n",
    "    \n",
    "    Try different domains like:\n",
    "    - \"cooking expert who loves Italian cuisine\"\n",
    "    - \"fitness trainer specializing in strength training\"\n",
    "    - \"travel guide for European destinations\"\n",
    "    - \"tech support specialist for beginners\"\n",
    "    \"\"\"\n",
    "    return ask_with_role(question, domain)\n",
    "\n",
    "# TODO: Modify the domain and question below\n",
    "specialist_domain = \"cooking expert who specializes in healthy meals\"\n",
    "specialist_question = \"What's a quick and healthy breakfast I can make in 10 minutes?\"\n",
    "\n",
    "answer = create_specialist_assistant(specialist_domain, specialist_question)\n",
    "print(f\"Domain: {specialist_domain}\")\n",
    "print(f\"Q: {specialist_question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Build a Simple Chatbot\n",
    "Create a basic conversational interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Simple chatbot with conversation history\n",
    "def simple_chatbot():\n",
    "    \"\"\"\n",
    "    A simple chatbot that maintains conversation context\n",
    "    Type 'quit' to exit\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    \n",
    "    print(\"🤖 Simple Chatbot initialized! Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"🤖 Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Add user message to history\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        try:\n",
    "            # Include conversation history for context\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=conversation_history,\n",
    "                temperature=0.7,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            bot_response = response.choices[0].message.content\n",
    "            print(f\"🤖 Bot: {bot_response}\\n\")\n",
    "            \n",
    "            # Add bot response to history\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"🤖 Error: {str(e)}\\n\")\n",
    "\n",
    "# Uncomment the line below to run the chatbot\n",
    "# simple_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Tips\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **API Key Security**: Never hardcode API keys. Use environment variables.\n",
    "\n",
    "2. **Temperature Settings**:\n",
    "   - Low (0.1-0.3): More focused, deterministic responses\n",
    "   - Medium (0.4-0.7): Balanced creativity and consistency\n",
    "   - High (0.8-1.0): More creative, varied responses\n",
    "\n",
    "3. **Prompt Engineering**:\n",
    "   - Be specific and clear\n",
    "   - Use examples (few-shot learning)\n",
    "   - Specify the desired format\n",
    "   - Use system messages for consistent behavior\n",
    "\n",
    "4. **Error Handling**: Always wrap API calls in try-except blocks\n",
    "\n",
    "5. **Cost Management**: Set appropriate `max_tokens` limits\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different models (gpt-4, gpt-3.5-turbo-16k)\n",
    "- Try function calling for structured outputs\n",
    "- Explore streaming responses for real-time applications\n",
    "- Learn about fine-tuning for specialized tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Challenges\n",
    "\n",
    "Try these advanced exercises:\n",
    "\n",
    "1. **Multi-turn Q&A**: Build a system that can answer follow-up questions based on previous context\n",
    "2. **Document Q&A**: Load a text document and build a system to answer questions about its content\n",
    "3. **Structured Output**: Create prompts that return responses in specific formats (JSON, tables, etc.)\n",
    "4. **Sentiment Analysis**: Use the LLM to analyze the sentiment of user inputs\n",
    "5. **Content Summarization**: Build a tool that summarizes long texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}